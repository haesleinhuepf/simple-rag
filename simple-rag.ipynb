{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9473c726-2de4-4791-9825-4c28fb4ebc96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple retrieval augmented generation\n",
    "In this notebook we see how retrieval augmented generation (RAG) works using OpenAI and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c88fd7-a683-4417-8090-705924ffde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0005d-a29f-4234-b40a-8f057aa0d535",
   "metadata": {},
   "source": [
    "We aim to answer this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cee2a87-dde6-4b8d-aafb-a146e2caf561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How can I label objects in an image?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd89ac-bd21-491d-84b8-42a26dd00afd",
   "metadata": {},
   "source": [
    "... using these code snippets (and more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d2d162-3818-480d-bbed-b427d3753b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('code_snippets.txt', 'r') as file:\n",
    "    all_code_snippets = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f201eec1-6ed0-4236-9594-286894574779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Displays an image with a slider and label showing mouse position and intensity.\n",
      "stackview.annotate(image, labels)\n",
      "* Allows cropping an image along all axes.\n",
      "stackview.crop(image)\n",
      "* Showing an image stored in variable `image` and a segmented image stored in variable `labels` on top. Also works with two images or two label images.\n",
      "stackview.curtain(image, labels, alpha: float = 1)\n"
     ]
    }
   ],
   "source": [
    "splits = all_code_snippets.split(\"\\n\\n\")\n",
    "_ = [print(s) for s in splits[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969aa87f-e7b1-49a2-9b1d-451e8d6434ba",
   "metadata": {},
   "source": [
    "## Vector embeddings\n",
    "To make our code snippets searchable, we need to embed them, we need to turn them into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7106fe-1e85-4146-807b-84ad64fea191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed(text):\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391a8185-d457-44cc-8ceb-3e014c7e5127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.002142224693670869, -0.04909248650074005, 0.02102646231651306], 1536)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embed(\"Hello world\")\n",
    "vector[:3], len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a249c-7571-4de9-b860-9c4f24fc1e94",
   "metadata": {},
   "source": [
    "## Vector store\n",
    "We also need a vector store, which is basically just a dictionary that allows us to quickly find a text given a corresponding vector, or a vector that has a short distance to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15a7142-1987-4f12-bbd6-5685a7f0c6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, texts=None):\n",
    "        self._store = {}\n",
    "        if texts is not None:\n",
    "            for text in texts:\n",
    "                self._store[tuple(embed(text))] = text\n",
    "    \n",
    "    def search(self, text, n_best_results=3):\n",
    "        single_vector = embed(text)\n",
    "        \n",
    "        # Step 1: Compute Euclidean distances\n",
    "        distances = [(np.linalg.norm(np.asarray(single_vector) - np.asarray(vector)), vector) for vector in self._store.keys()]\n",
    "\n",
    "        # Step 2: Sort distances and get the three vectors with the shortest distances\n",
    "        distances.sort()  # Sort based on the first element in the tuple (distance)\n",
    "        closest_vectors = [vec for _, vec in distances[:n_best_results]]  # Extract only the vectors\n",
    "\n",
    "        self.distances = distances\n",
    "        \n",
    "        return [self._store[tuple(v)] for v in closest_vectors]\n",
    "    \n",
    "    def get_text(self, vector):\n",
    "        return self._store[vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a321b23-b6bc-44fb-a4c6-0b273f1ec69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectore_store = VectorStore(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39b4fa-596b-4039-97eb-27b02c7fb431",
   "metadata": {},
   "source": [
    "## Searching the vector store\n",
    "We can then search in the store for vectors and corresponding texts that are close by a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7f8d20-21be-4573-ba20-d9ff1557ab74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I label objects in an image?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54f30f2-235d-40dd-8282-8ce794e35ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['* Labels objects in grey-value images using Gaussian blurs, spot detection, Otsu-thresholding, and Voronoi-labeling from isotropic input images.\\ncle.voronoi_otsu_labeling(source: ndarray, label_image_destination: ndarray = None, spot_sigma: float = 2, outline_sigma: float = 2) -> ndarray',\n",
       " '* Apply morphological opening operation, fill label gaps with voronoi-labeling, and mask background pixels in label image.\\ncle.smooth_labels(labels_input: ndarray, labels_destination: ndarray = None, radius: int = 0) -> ndarray',\n",
       " '* Dilates labels in an isotropic label image without overwriting other labels.\\ncle.dilate_labels(labeling_source: ndarray, labeling_destination: ndarray = None, radius: int = 2) -> ndarray']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_snippets = vectore_store.search(question)\n",
    "code_snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48968620-c770-4069-82af-e77e4e79ae86",
   "metadata": {},
   "source": [
    "## Prompting OpenAI\n",
    "We will also need access to a large language model (LLM) to combine the code snippets and the question to retrieve an answer to our question that involves the code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f95c50-a6c8-40c7-8634-e6cb687c7582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_chatGPT(message:str, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"A prompt helper function that sends a message to openAI\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import openai\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    # todo: enter your API key here:\n",
    "    client = openai.OpenAI(api_key = os.environ.get('OPENAI_API_KEY'))\n",
    "    \n",
    "    # submit prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c9e35-0ebb-46a0-a4f6-5fd4f32692aa",
   "metadata": {},
   "source": [
    "We can then assemble code snippets and question to a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13413154-ff5b-4c75-8a38-833d4eb63b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question by the very end and take given code snippets into account.\n",
      "\n",
      "## Code snippets\n",
      "* Labels objects in grey-value images using Gaussian blurs, spot detection, Otsu-thresholding, and Voronoi-labeling from isotropic input images.\n",
      "cle.voronoi_otsu_labeling(source: ndarray, label_image_destination: ndarray = None, spot_sigma: float = 2, outline_sigma: float = 2) -> ndarray\n",
      "\n",
      "* Apply morphological opening operation, fill label gaps with voronoi-labeling, and mask background pixels in label image.\n",
      "cle.smooth_labels(labels_input: ndarray, labels_destination: ndarray = None, radius: int = 0) -> ndarray\n",
      "\n",
      "* Dilates labels in an isotropic label image without overwriting other labels.\n",
      "cle.dilate_labels(labeling_source: ndarray, labeling_destination: ndarray = None, radius: int = 2) -> ndarray\n",
      "\n",
      "## Question\n",
      "How can I label objects in an image?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\\n\".join(code_snippets)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the question by the very end and take given code snippets into account.\n",
    "\n",
    "## Code snippets\n",
    "{context}\n",
    "\n",
    "## Question\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfb432-97e5-40fa-b151-288ed9340983",
   "metadata": {},
   "source": [
    "## Answering our question\n",
    "Eventually we can answer our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4471a899-a6c2-4de0-9a33-6b0731f2140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can label objects in an image by using the provided code snippets. First, you can use `cle.voronoi_otsu_labeling` to label objects in grey-value images using Gaussian blurs, spot detection, Otsu-thresholding, and Voronoi-labeling. Then, you can apply morphological operations and fill label gaps with voronoi-labeling using `cle.smooth_labels`. Finally, you can dilate the labels in the image without overwriting other labels using `cle.dilate_labels`. By combining these steps, you can effectively label objects in an image.\n"
     ]
    }
   ],
   "source": [
    "answer = prompt_chatGPT(prompt)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f1e51-3eed-47e3-accd-5441dff8f0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
